{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOJ6eRGNVQkCAjNiyw3JJj0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuF1jWeGxwVi","executionInfo":{"status":"ok","timestamp":1737975092608,"user_tz":-60,"elapsed":368,"user":{"displayName":"Mikael Poli","userId":"13703176784544242046"}},"outputId":"fc283b5f-aab1-4a62-f3d0-21300b64496c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing model_training.py\n"]}],"source":["%%writefile model_training.py\n","import pandas as pd\n","import tensorflow as tf\n","import numpy as np\n","import keras\n","from functools import partial\n","from tensorflow.keras.models import Model, Sequential, load_model\n","from tensorflow.keras.layers import Input, BatchNormalization, Dense, Dropout, ReLU, LeakyReLU\n","from tensorflow.keras.initializers import HeNormal\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import KFold, train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","import matplotlib.pyplot as plt\n","\n","def build_model(input_dim, output_dim, aspect, model_type, n_hidden, size_hidden, activation_hidden, dropout, leaky_relu_alpha, set_seed=42):\n","  \"\"\"\n","  Args:\n","    model_type (str) can be 'decreasing' or 'fixed'. \"decreasing\" will create an architecture where evey hidden layer is half the size of its predecessor. \"fixed\"\n","    will create an architecture where all hidden layers have the same size.\n","    aspect (str) can be 'molecular_function', 'biological_process', or 'cellular_component'\n","    activation_hidden (str) can be 'relu' or 'leaky_relu'\n","  \"\"\"\n","  # Ensure model type is specified\n","  if model_type not in ('decreasing', 'fixed'):\n","      raise ValueError(\"Model type must be 'decreasing' or 'fixed'\")\n","  # Ensure aspect is specified\n","  if aspect not in ('molecular_function', 'biological_process', 'cellular_component'):\n","      raise ValueError(\"Aspect must be 'molecular_function' or 'biological_process' or 'cellular_component'\")\n","  # Ensure activation function is specified\n","  if activation_hidden not in ('relu', 'leaky_relu'):\n","      raise ValueError(\"Activation for hidden layers must be 'relu' or 'leaky_relu'\")\n","\n","  # Input, dropout, and batch normalization layers\n","  inputs = Input(shape=(input_dim,), name='input_layer')\n","  x = BatchNormalization(name='layer_0_batch_norm')(inputs)\n","  x = Dropout(dropout, name='layer_0_dropout')(x)\n","\n","  # Hidden layers for decreasing model\n","  if model_type == 'decreasing':\n","    dense_layer = partial(Dense, kernel_initializer=HeNormal(seed=set_seed), use_bias=True)\n","    for i in range(n_hidden):\n","        x = dense_layer(size_first_hidden, name=f'layer_{i+1}_dense')(x)\n","        if activation_hidden == 'relu':\n","          x = ReLU(name=f'layer_{i+1}_relu')(x)\n","        else:\n","          x = LeakyReLU(negative_slope=leaky_relu_alpha, name=f'layer_{i+1}_leaky_relu')(x)\n","        x = BatchNormalization(name=f'layer_{i+1}_batch_norm')(x)\n","        x = Dropout(dropout, name=f'layer_{i+1}_dropout')(x)\n","        size_first_hidden = size_first_hidden // 2\n","\n","  # Hidden layers for fixed model\n","  elif model_type == 'fixed':\n","    dense_layer = partial(Dense, kernel_initializer=HeNormal(seed=set_seed), use_bias=True)\n","    for i in range(n_hidden):\n","        x = dense_layer(size_hidden, name=f'layer_{i+1}_dense')(x)\n","        if activation_hidden == 'relu':\n","          x = ReLU(name=f'layer_{i+1}_relu')(x)\n","        else:\n","          x = LeakyReLU(negative_slope=leaky_relu_alpha, name=f'layer_{i+1}_leaky_relu')(x)\n","        x = BatchNormalization(name=f'layer_{i+1}_batch_norm')(x)\n","        x = Dropout(dropout, name=f'layer_{i+1}_dropout')(x)\n","\n","  # Output layer\n","  output = Dense(output_dim, activation='sigmoid', name='output_layer_sigmoid')(x)\n","\n","  # Create the model\n","  model = Model(inputs=inputs, outputs=output, name=f'{aspect}_{model_type}')\n","  return model\n","\n","def plot_metrics(history):\n","    # Plot loss\n","    plt.figure(figsize=(15, 5))\n","    plt.subplot(1, 3, 1)  # 1 row, 3 columns; first plot\n","    plt.plot(history.history['loss'], label='Train Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.title('Loss Over Epochs')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    # Plot AUC-ROC\n","    plt.subplot(1, 3, 2)  # 1 row, 3 columns; second plot\n","    plt.plot(history.history['auc-roc'], label='Train AUC-ROC')\n","    plt.plot(history.history['val_auc-roc'], label='Validation AUC-ROC')\n","    plt.title('AUC ROC Over Epochs')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('AUC-ROC')\n","    plt.legend()\n","\n","    # Plot AUC-PR\n","    plt.subplot(1, 3, 3)  # 1 row, 3 columns; third plot\n","    plt.plot(history.history['auc-pr'], label='Train AUC-PR')\n","    plt.plot(history.history['val_auc-pr'], label='Validation AUC-PR')\n","    plt.title('AUC Precision-Recall Over Epochs')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('AUC-PR')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def train_model(X_train, y_train, X_val, y_val, aspect, model_type, n_hidden, size_hidden, activation, dropout, learning_rate, epochs, batch_size, leaky_relu_alpha=0.01, set_seed=42):\n","  # Ensure type of model is provided\n","  if model_type not in ('decreasing', 'fixed'):\n","      raise ValueError(\"Type must be 'decreasing' or 'fixed'\")\n","  # Ensure activation function is provided\n","  if activation not in ('relu', 'leaky_relu'):\n","      raise ValueError(\"Type must be 'relu' or 'leaky_relu'\")\n","  keras.utils.set_random_seed(set_seed)\n","  input_dim = X_train.shape[1]\n","  output_dim = y_train.shape[1]\n","\n","  # Build model\n","  model = build_model(input_dim, output_dim, aspect, model_type, n_hidden, size_hidden, activation, dropout, leaky_relu_alpha, set_seed)\n","\n","  # Print model configuration\n","  print(\"=\" * 40)\n","  print(f\"{'Model Configuration':^40}\")\n","  print(\"=\" * 40)\n","  print(f\"Aspect:               {aspect}\")\n","  print(f\"Model Type:           {model_type}\")\n","  print(f\"Hidden Layers:        {n_hidden}\")\n","  if model_type == 'decreasing':\n","      print(f\"First Layer Units:    {size_hidden}\")\n","  elif model_type == 'fixed':\n","      print(f\"Units Per Layer:      {size_hidden}\")\n","  print(f\"Dropout:              {dropout}\")\n","  print(f\"Activation:           {activation}\")\n","  if activation == 'leaky_relu':\n","      print(f\"Leaky ReLU Alpha:     {leaky_relu_alpha}\")\n","  print(f\"Learning Rate:        {learning_rate}\")\n","  print(\"=\" * 40)\n","\n","  # Implement early stopping and adaptive learning rate\n","  early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n","  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n","\n","  model.compile(\n","  loss='binary_crossentropy',\n","  optimizer=Adam(learning_rate=learning_rate),\n","  metrics=[\n","      tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\"),\n","      tf.keras.metrics.Precision(name=\"precision\"),\n","      tf.keras.metrics.Recall(name=\"recall\"),\n","      tf.keras.metrics.AUC(curve='ROC', name=\"auc-roc\", multi_label=True),\n","      tf.keras.metrics.AUC(curve='PR', name=\"auc-pr\", multi_label=True)\n","      ]\n","  )\n","\n","  # Record performance\n","  model.history = model.fit(\n","      X_train,\n","      y_train,\n","      validation_data=(X_val, y_val),\n","      epochs=epochs,\n","      batch_size=batch_size,\n","      callbacks=[early_stopping, reduce_lr],\n","      verbose=1\n","  )\n","\n","  # Plot training and validation metrics\n","  plot_metrics(model.history)\n","  return model\n","\n","def plot_label_distribution(train_labels, val_labels, aspect, bin_size=30):\n","  # Count label frequencies\n","  train_label_distribution = np.sum(train_labels, axis=0)\n","  test_label_distribution = np.sum(val_labels, axis=0)\n","\n","  # Calculate the number of bins to ensure divisibility\n","  num_bins = train_label_distribution.shape[0] // bin_size\n","\n","  # Reshape the distributions based on the calculated number of bins\n","  train_bins = np.sum(train_label_distribution[:num_bins * bin_size].reshape(-1, bin_size), axis=1)\n","  val_bins = np.sum(test_label_distribution[:num_bins * bin_size].reshape(-1, bin_size), axis=1)\n","  bin_indices = np.arange(len(train_bins))\n","\n","  plt.bar(bin_indices - 0.2, train_bins, width=0.4, label='Train', color='blue', alpha=0.7)\n","  plt.bar(bin_indices + 0.2, val_bins, width=0.4, label='Val', color='orange', alpha=0.7)\n","  plt.title(f\"Binned Label Distribution for {aspect}\")\n","  plt.xlabel('Label Bins')\n","  plt.ylabel('Frequency')\n","  plt.legend()\n","  plt.grid(axis='y', linestyle='--', alpha=0.7)\n","  plt.tight_layout()\n","  plt.show()\n","\n","def build_predictions_df(predictions_array, protein_ids, classes, aspect):\n","  protein_id_list = []\n","  aspect_list = []\n","  go_term_list = []\n","  prediction_list = []\n","\n","  # Loop through each protein and its predictions\n","  for i, protein_id in enumerate(protein_ids):\n","      # For each protein, loop through the GO terms\n","      for j, go_term in enumerate(classes):\n","          protein_id_list.append(protein_id)\n","          aspect_list.append(aspect)  # All rows will have the same aspect\n","          go_term_list.append(go_term)\n","          prediction_list.append(predictions_array[i, j])\n","\n","  # Create the DataFrame\n","  df = pd.DataFrame({\n","      'protein_id': protein_id_list,\n","      'aspect': aspect_list,\n","      'go_term': go_term_list,\n","      'prediction': prediction_list\n","  })\n","  return df"]}]}