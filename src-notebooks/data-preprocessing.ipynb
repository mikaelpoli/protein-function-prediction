{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN68rXaHTOiy+6ZyUGI/Yhl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O2WWSiP2MKgv","executionInfo":{"status":"ok","timestamp":1737974733999,"user_tz":-60,"elapsed":398,"user":{"displayName":"Mikael Poli","userId":"13703176784544242046"}},"outputId":"8e3a827c-335c-4d78-b552-72c8c88d9aa0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing data_preprocessing.py\n"]}],"source":["%%writefile data_preprocessing.py\n","import pandas as pd\n","import numpy as np\n","import h5py\n","from sklearn.preprocessing import MultiLabelBinarizer\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","\"\"\"\n","Datasets\n","\"\"\"\n","\n","def load_original_data(config, test=False):\n","  # Set path\n","  embeddings_path = config['paths']['test_embeddings'] if test else config['paths']['train_embeddings']\n","\n","  # Load T5 protein embeddings from a .h5 file\n","  with h5py.File(embeddings_path, 'r') as f:\n","\n","    # Extrect protein IDs\n","    ids = np.array(list(f.keys()))\n","    embeddings = np.array([f[protein_id][:] for protein_id in ids])\n","\n","  if not test:\n","    # Load train set from a .tsv file\n","    train_set = pd.read_csv(config['paths']['train_set'], sep='\\t')\n","    train_set.rename(columns={'Protein_ID': 'protein_id', 'GO_term': 'go_term'}, inplace=True)\n","\n","    # Sort train set alphabetically (like protein embeddings)\n","    train_set.sort_values(by=['protein_id'], inplace=True)\n","\n","    return train_set, embeddings, ids\n","\n","  return embeddings, ids\n","\n","def load_custom_data(config, test=False):\n","  # Set paths\n","  embeddings_path = config['paths']['custom_test_embeddings'] if test else config['paths']['custom_train_embeddings']\n","  embeddings_key = 'custom_test_embeddings' if test else 'custom_train_embeddings'\n","  dataset_path = config['paths']['custom_test_set'] if test else config['paths']['custom_train_set']\n","  ids_path = config['paths']['custom_test_ids'] if test else config['paths']['custom_train_ids']\n","\n","  # Load T5 protein embeddings from a .h5 file\n","  with h5py.File(embeddings_path, 'r') as f:\n","    embeddings = np.array(f[embeddings_key])\n","\n","  # Load dataset and IDs from a .tsv file\n","  dataset = pd.read_csv(dataset_path, sep='\\t')\n","  ids_df = pd.read_csv(ids_path, sep='\\t', header=None)\n","  ids = ids_df[0].to_numpy()\n","\n","  return dataset, embeddings, ids\n","\n","\"\"\"\n","Preprocessing\n","\"\"\"\n","\n","def get_protein_ids(df):\n","  return df['protein_id'].unique()\n","\n","def split_by_aspect(df):\n","  df_mf = df[df['aspect'] == 'molecular_function']\n","  df_bp = df[df['aspect'] == 'biological_process']\n","  df_cc = df[df['aspect'] == 'cellular_component']\n","  return df_mf, df_bp, df_cc\n","\n","# Prepare training set labels\n","def get_labels_dict(df):\n","  labels_dict = df.groupby('protein_id')['go_term'].apply(list).to_dict()\n","  return labels_dict\n","\n","\n","# Binarize training labels for multi-label classifiction\n","def binarize_labels(labels_dict):\n","  labels_list = list(labels_dict.values())\n","  mlb = MultiLabelBinarizer()\n","  labels_bin = mlb.fit_transform(labels_list)\n","  return labels_bin, mlb.classes_\n","\n","\"\"\"\n","Data prparation for DNN training\n","\"\"\"\n","def build_custom_datasets(train_ids, train_embeddings, custom_test_ids):\n","  # Get the custom test set protein embeddings and indices\n","  sampled_indices = np.isin(train_ids, custom_test_ids)\n","  custom_test_idx = np.where(sampled_indices)[0]\n","  custom_test_embeddings = train_embeddings[custom_test_idx]\n","\n","  # Get the updated training set protein embeddings, indices, and protein IDs\n","  custom_train_idx = np.where(~sampled_indices)[0]\n","  custom_train_embeddings = train_embeddings[custom_train_idx]\n","  custom_train_ids = train_ids[~np.isin(train_ids, custom_test_ids)]\n","\n","  return custom_test_idx, custom_test_embeddings, custom_train_ids, custom_train_idx, custom_train_embeddings\n","\n","# Prepare input features and labels for each custom aspect training set\n","def prepare_features_and_labels(aspect, custom_aspect_dataset, custom_train_embeddings, custom_full_train_ids):\n","  print(f\"Preparing {aspect} features and labels\")\n","\n","  # Locate indices in training set\n","  id_set = set(custom_aspect_dataset['protein_id'].unique())\n","  idx = np.array([id_ in id_set for id_ in custom_full_train_ids])\n","\n","  # Get indices from training set\n","  custom_train_aspect_idx = np.where(idx)[0]\n","  custom_train_aspect_ids = custom_full_train_ids[custom_train_aspect_idx]\n","\n","  # Get protein embeddings corresponding to indices and save ID-embedding correspondence\n","  x = custom_train_embeddings[custom_train_aspect_idx]\n","  custom_train_aspect_dict = dict(zip(custom_train_aspect_ids, x))\n","\n","  # Prepare labels\n","  y = get_labels_dict(custom_aspect_dataset)\n","  y_bin, classes = binarize_labels(y)\n","\n","  return x, y_bin, classes\n","\n","def plot_label_distribution(train_labels, val_labels, aspect, bin_size=30):\n","  # Count label frequencies\n","  train_label_distribution = np.sum(train_labels, axis=0)\n","  test_label_distribution = np.sum(val_labels, axis=0)\n","\n","  # Calculate the number of bins to ensure divisibility\n","  num_bins = train_label_distribution.shape[0] // bin_size\n","\n","  # Reshape the distributions based on the calculated number of bins\n","  train_bins = np.sum(train_label_distribution[:num_bins * bin_size].reshape(-1, bin_size), axis=1)\n","  val_bins = np.sum(test_label_distribution[:num_bins * bin_size].reshape(-1, bin_size), axis=1)\n","  bin_indices = np.arange(len(train_bins))\n","\n","  plt.bar(bin_indices - 0.2, train_bins, width=0.4, label='Train', color='blue', alpha=0.7)\n","  plt.bar(bin_indices + 0.2, val_bins, width=0.4, label='Val', color='orange', alpha=0.7)\n","  plt.title(f\"Binned Label Distribution for {aspect}\")\n","  plt.xlabel('Label Bins')\n","  plt.ylabel('Frequency')\n","  plt.legend()\n","  plt.grid(axis='y', linestyle='--', alpha=0.7)\n","  plt.tight_layout()\n","  plt.show()\n","\n","  return plt\n","\n","\"\"\"\n","Data preparation for BLAST-based scores\n","\"\"\"\n","# Process GO terms from the training dataset.\n","def process_go_terms(train_set):\n","  go_terms = [[term] for term in train_set['go_term'].unique()]\n","  sorted_go_terms = sorted(go_terms)\n","\n","  mlb = MultiLabelBinarizer()\n","  binary_go_matrix = mlb.fit_transform(sorted_go_terms)\n","\n","  # Create a dictionary with GO term labels as keys and binary vectors as values\n","  go_terms_dict = {\n","      go_term[0]: np.array(binary_vector.tolist())\n","      for go_term, binary_vector in zip(sorted_go_terms, binary_go_matrix)\n","  }\n","\n","  return go_terms_dict\n","\n","# Process BLAST results by filtering, weighting, and normalizing\n","def process_blast_results(train_blast, custom_test_ids, evalue_threshold=1e-5, top_n=50, max_value=10):\n","  \"\"\"\n","  Args:\n","      train_blast (pd.DataFrame): A DataFrame containing BLAST results with columns 'query', 'target', and 'evalue'.\n","      custom_test_ids (set): A set of protein IDs in the custom test set.\n","      evalue_threshold (float, optional): The maximum e-value threshold for filtering. Default is 1e-5.\n","      top_n (int, optional): The maximum number of matches to keep for each query protein. Default is 50.\n","      max_value (float, optional): The maximum weight value. Default is 10.\n","\n","  Returns:\n","      pd.DataFrame: A processed DataFrame with filtered, weighted, and normalized BLAST results.\n","  \"\"\"\n","  # Keep BLAST results only for the proteins in the custom test set\n","  blast = train_blast[train_blast[\"query\"].isin(custom_test_ids)]\n","\n","  # Remove BLAST results where the target protein is in the custom test set\n","  blast = blast[~blast[\"target\"].isin(custom_test_ids)]\n","\n","  # Filter BLAST results based on e-value and keep only the top N matches\n","  blast = blast[blast[\"evalue\"] <= evalue_threshold]\n","  blast = (\n","      blast.sort_values(by=[\"query\", \"evalue\"])\n","      .groupby(\"query\")\n","      .head(top_n)\n","  )\n","\n","  # Calculate weights based on e-value\n","  blast.loc[:, \"weight\"] = np.minimum(\n","      -np.log10(blast.loc[:, \"evalue\"] + 1e-300), 1e6\n","  )\n","\n","  # Normalize weights for each query protein\n","  blast[\"weight\"] = blast.groupby(\"query\")[\"weight\"].transform(\n","      lambda x: x / x.sum() * max_value\n","  )\n","\n","  return blast\n","\n","# Create a dictionary mapping BLAST target protein IDs to their corresponding GO terms\n","def create_target_go_terms_dict(target_go_terms):\n","  \"\"\"\n","  Args:\n","      target_go_terms (pd.DataFrame): A DataFrame where the index contains protein IDs\n","                                      and the rows contain GO terms.\n","  \"\"\"\n","  return {\n","      protein_id: target_go_terms.iloc[i]\n","      for i, protein_id in enumerate(target_go_terms.index)\n","  }\n","\n","\"\"\"\n","Process predictions for .tsv export\n","\"\"\"\n","\n","def process_predictions(combined_predictions, aspect, threshold=0.2, top_n=500):\n","  \"\"\"\n","  Args:\n","      combined_predictions (pd.DataFrame): Input dataframe.\n","      threshold (float): Minimum prediction value to retain rows.\n","      top_n (int): Number of top predictions to retain per group.\n","\n","  Returns:\n","      pd.DataFrame: Processed dataframe.\n","  \"\"\"\n","  df = combined_predictions[combined_predictions['aspect']==aspect]\n","  df_sorted = df.sort_values(by=['protein_id', 'prediction'], ascending=[True, False])\n","  df_limited = df_sorted.groupby('protein_id').head(top_n)\n","  df_limited.loc[:, 'prediction'] = df_limited['prediction'].round(3)\n","\n","  return df_limited[df_limited['prediction'] > threshold]"]}]}